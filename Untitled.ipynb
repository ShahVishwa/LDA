{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'a', u'about', u'above', u'after', u'again', u'against', u'all', u'am', u'an', u'and', u'any', u'are', u\"aren't\", u'as', u'at', u'be', u'because', u'been', u'before', u'being', u'below', u'between', u'both', u'but', u'by', u\"can't\", u'cannot', u'could', u\"couldn't\", u'did', u\"didn't\", u'do', u'does', u\"doesn't\", u'doing', u\"don't\", u'down', u'during', u'each', u'few', u'for', u'from', u'further', u'had', u\"hadn't\", u'has', u\"hasn't\", u'have', u\"haven't\", u'having', u'he', u\"he'd\", u\"he'll\", u\"he's\", u'her', u'here', u\"here's\", u'hers', u'herself', u'him', u'himself', u'his', u'how', u\"how's\", u'i', u\"i'd\", u\"i'll\", u\"i'm\", u\"i've\", u'if', u'in', u'into', u'is', u\"isn't\", u'it', u\"it's\", u'its', u'itself', u\"let's\", u'me', u'more', u'most', u\"mustn't\", u'my', u'myself', u'no', u'nor', u'not', u'of', u'off', u'on', u'once', u'only', u'or', u'other', u'ought', u'our', u'ours', u'ourselves', u'out', u'over', u'own', u'same', u\"shan't\", u'she', u\"she'd\", u\"she'll\", u\"she's\", u'should', u\"shouldn't\", u'so', u'some', u'such', u'than', u'that', u\"that's\", u'the', u'their', u'theirs', u'them', u'themselves', u'then', u'there', u\"there's\", u'these', u'they', u\"they'd\", u\"they'll\", u\"they're\", u\"they've\", u'this', u'those', u'through', u'to', u'too', u'under', u'until', u'up', u'very', u'was', u\"wasn't\", u'we', u\"we'd\", u\"we'll\", u\"we're\", u\"we've\", u'were', u\"weren't\", u'what', u\"what's\", u'when', u\"when's\", u'where', u\"where's\", u'which', u'while', u'who', u\"who's\", u'whom', u'why', u\"why's\", u'with', u\"won't\", u'would', u\"wouldn't\", u'you', u\"you'd\", u\"you'll\", u\"you're\", u\"you've\", u'your', u'yours', u'yourself', u'yourselves']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "print(en_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create sample documents\n",
    "doc_a = \"Tis the Season for Christmas Cookies!  My all time favorite Christmas Cookie is, and always has been, the Peanut Butter Kiss Cookie.  However, I have cut peanut butter, sugar and crap out of my life.  But it’s Christmas y’all!  If you’re going to sneak a treat, this is the time to do it!  That being said, I decided to peace out the peanut butter cookie and make a Paleo Almond Butter Cookie!\"\n",
    "doc_b = \"I grew up with a heart for travel.  Then I grew up, got a job and realized I have also heart for vacation.  I’m the kind of traveler who is happy with a trusty backpack, comfy sneakers, a camera and a eurail train pass… I’m also one who won’t say no to luxury hotels, beach views and a fruity beverages.\"\n",
    "doc_c = \"If it were at all possible, I would pay anything and everything to rewind the clock to exactly one year ago today: November 9, 2013.  A day full of glitter and details.  A day of love, joy, and plenty of happy tears.  A day my creative party planning skills were at their finest.  A day when every single person who means the world to me, were all in the exact same room.  A day I partied like a rockstar on cold medicine.  The day I said “I do” to the love of my life.\"\n",
    "doc_d = \"It’s been one month now with the house on the market.  I would by lying if I said I wasn’t on edge about it.  Between keeping it clean and staying nearby just incase I have to swoop up the dogs for a showing… it’s just high stress.  Add to it that we have not had ONE SINGLE SHOWING!  I’m not blaming my realtor one bit.  She’s a rockstar.  She is also my mom and is also not taking a commission if when the house finally sells.  I blame the weather.  We have either had snow on the ground or temps below 40 every day this month.  Only crazy people buy houses in this weather.  Albeit I’m totally okay with selling to anyone, crazy or not.\"\n",
    "doc_e = \"As a professional event planner, I know every shindig needs a Plan A and a Plan B.  Our Plan A included the back deck, a fire pit, tons of space for tons of guests, a canvas in the middle of the yard, sunshine and eggs filled with either blue or pink paint.  Plan B?  Well Plan B involved a cake with either pink or blue on the inside and a lot less space in the house… My parents knew how much Plan A meant to me, so when the forecast was terrible we tried to work around it!\"\n",
    "doc_f = \"Two years ago, he experienced the lowest of the lows, but now Robert Herjavec is bouncing back from a painful divorce – and moving on with his brand-new fiancée, his former Dancing with the Stars partner Kym Johnson. But it wasn't an easy road to his new-found happiness. \"\n",
    "doc_g = \"Chere Rush was a 39-year-old mother with a 10-year-old and 8-year-old twins when she first noticed a small lump in her breast. \"\n",
    "doc_h = \"Redmond police on Wednesday night confirmed that Seahawks safety Kam Chancellor was involved in an incident that led to a 911 call at the Redmond Athletic Club.\"\n",
    "doc_i = \"The grocery chain, which previously accepted only cash and debit cards, is now accepting all major credit cards, including Visa, MasterCard, Discover, and American Express.\"\n",
    "doc_j = \"A new witness has come forward saying he was in the looker room and saw the event at the heart of new sexual-assault allegations against Peyton Manning involving then-University of Tennessee trainer Jamie Naughright.\"\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e, doc_f, doc_g, doc_h, doc_i, doc_j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'ti', u'season', u'christma', u'cooki', u'time', u'favorit', u'christma', u'cooki', u'alway', u'peanut', u'butter', u'kiss', u'cooki', u'howev', u'cut', u'peanut', u'butter', u'sugar', u'crap', u'life', u's', u'christma', u'y', u're', u'go', u'sneak', u'treat', u'time', u'said', u'decid', u'peac', u'peanut', u'butter', u'cooki', u'make', u'paleo', u'almond', u'butter', u'cooki'], [u'grew', u'heart', u'travel', u'grew', u'got', u'job', u'realiz', u'also', u'heart', u'vacat', u'm', u'kind', u'travel', u'happi', u'trusti', u'backpack', u'comfi', u'sneaker', u'camera', u'eurail', u'train', u'pass', u'm', u'also', u'one', u'won', u't', u'say', u'luxuri', u'hotel', u'beach', u'view', u'fruiti', u'beverag'], [u'possibl', u'pay', u'anyth', u'everyth', u'rewind', u'clock', u'exactli', u'one', u'year', u'ago', u'today', u'novemb', u'9', u'2013', u'day', u'full', u'glitter', u'detail', u'day', u'love', u'joy', u'plenti', u'happi', u'tear', u'day', u'creativ', u'parti', u'plan', u'skill', u'finest', u'day', u'everi', u'singl', u'person', u'mean', u'world', u'exact', u'room', u'day', u'parti', u'like', u'rockstar', u'cold', u'medicin', u'day', u'said', u'love', u'life'], [u's', u'one', u'month', u'now', u'hous', u'market', u'lie', u'said', u'wasn', u't', u'edg', u'keep', u'clean', u'stay', u'nearbi', u'just', u'incas', u'swoop', u'dog', u'show', u's', u'just', u'high', u'stress', u'add', u'one', u'singl', u'show', u'm', u'blame', u'realtor', u'one', u'bit', u's', u'rockstar', u'also', u'mom', u'also', u'take', u'commiss', u'hous', u'final', u'sell', u'blame', u'weather', u'either', u'snow', u'ground', u'temp', u'40', u'everi', u'day', u'month', u'crazi', u'peopl', u'buy', u'hous', u'weather', u'albeit', u'm', u'total', u'okay', u'sell', u'anyon', u'crazi'], [u'profession', u'event', u'planner', u'know', u'everi', u'shindig', u'need', u'plan', u'plan', u'b', u'plan', u'includ', u'back', u'deck', u'fire', u'pit', u'ton', u'space', u'ton', u'guest', u'canva', u'middl', u'yard', u'sunshin', u'egg', u'fill', u'either', u'blue', u'pink', u'paint', u'plan', u'b', u'well', u'plan', u'b', u'involv', u'cake', u'either', u'pink', u'blue', u'insid', u'lot', u'less', u'space', u'hous', u'parent', u'knew', u'much', u'plan', u'meant', u'forecast', u'terribl', u'tri', u'work', u'around'], [u'two', u'year', u'ago', u'experienc', u'lowest', u'low', u'now', u'robert', u'herjavec', u'bounc', u'back', u'pain', u'divorc', u'move', u'brand', u'new', u'fianc', u'e', u'former', u'danc', u'star', u'partner', u'kym', u'johnson', u'wasn', u't', u'easi', u'road', u'new', u'found', u'happi'], [u'chere', u'rush', u'39', u'year', u'old', u'mother', u'10', u'year', u'old', u'8', u'year', u'old', u'twin', u'first', u'notic', u'small', u'lump', u'breast'], [u'redmond', u'polic', u'wednesday', u'night', u'confirm', u'seahawk', u'safeti', u'kam', u'chancellor', u'involv', u'incid', u'led', u'911', u'call', u'redmond', u'athlet', u'club'], [u'groceri', u'chain', u'previous', u'accept', u'cash', u'debit', u'card', u'now', u'accept', u'major', u'credit', u'card', u'includ', u'visa', u'mastercard', u'discov', u'american', u'express'], [u'new', u'wit', u'come', u'forward', u'say', u'looker', u'room', u'saw', u'event', u'heart', u'new', u'sexual', u'assault', u'alleg', u'peyton', u'man', u'involv', u'univers', u'tennesse', u'trainer', u'jami', u'naughright']]\n"
     ]
    }
   ],
   "source": [
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "    \n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 3), (6, 1), (7, 3), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 5), (18, 1), (19, 1), (20, 1), (21, 4), (22, 1), (23, 1), (24, 1), (25, 2), (26, 1)], [(27, 2), (28, 1), (29, 2), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 2), (36, 1), (37, 2), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 2), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1)], [(13, 1), (18, 1), (31, 1), (36, 1), (56, 1), (57, 1), (58, 1), (59, 2), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 6), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 2), (91, 1), (92, 1)], [(18, 1), (24, 3), (29, 2), (31, 3), (37, 2), (53, 1), (60, 1), (74, 1), (81, 1), (89, 1), (93, 1), (94, 2), (95, 2), (96, 2), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1), (102, 1), (103, 1), (104, 1), (105, 2), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 2), (121, 1), (122, 1), (123, 1), (124, 1), (125, 1), (126, 1), (127, 1), (128, 3), (129, 1), (130, 1), (131, 1), (132, 1)], [(74, 1), (77, 6), (128, 1), (131, 2), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 2), (142, 1), (143, 1), (144, 2), (145, 1), (146, 1), (147, 1), (148, 1), (149, 1), (150, 1), (151, 1), (152, 2), (153, 1), (154, 2), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 3), (164, 1), (165, 1), (166, 1), (167, 1), (168, 1), (169, 1), (170, 1), (171, 1)], [(36, 1), (53, 1), (63, 1), (83, 1), (100, 1), (123, 1), (135, 1), (172, 1), (173, 1), (174, 1), (175, 1), (176, 1), (177, 1), (178, 1), (179, 1), (180, 1), (181, 2), (182, 1), (183, 1), (184, 1), (185, 1), (186, 1), (187, 1), (188, 1), (189, 1), (190, 1), (191, 1), (192, 1), (193, 1), (194, 1)], [(63, 3), (195, 1), (196, 1), (197, 1), (198, 3), (199, 1), (200, 1), (201, 1), (202, 1), (203, 1), (204, 1), (205, 1), (206, 1), (207, 1)], [(166, 1), (208, 2), (209, 1), (210, 1), (211, 1), (212, 1), (213, 1), (214, 1), (215, 1), (216, 1), (217, 1), (218, 1), (219, 1), (220, 1), (221, 1), (222, 1)], [(123, 1), (150, 1), (223, 1), (224, 1), (225, 1), (226, 1), (227, 2), (228, 1), (229, 1), (230, 1), (231, 1), (232, 1), (233, 1), (234, 1), (235, 2), (236, 1)], [(27, 1), (32, 1), (82, 1), (139, 1), (166, 1), (181, 2), (237, 1), (238, 1), (239, 1), (240, 1), (241, 1), (242, 1), (243, 1), (244, 1), (245, 1), (246, 1), (247, 1), (248, 1), (249, 1), (250, 1), (251, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.029*redmond + 0.029*new + 0.015*involv + 0.015*confirm + 0.015*chancellor'), (1, u'0.035*also + 0.035*m + 0.035*travel + 0.035*heart + 0.035*grew'), (2, u'0.034*s + 0.034*one + 0.034*hous + 0.023*month + 0.023*sell'), (3, u'0.044*new + 0.023*forward + 0.023*alleg + 0.023*jami + 0.023*peyton'), (4, u'0.062*cooki + 0.050*butter + 0.038*christma + 0.038*peanut + 0.026*time'), (5, u'0.076*plan + 0.039*b + 0.026*either + 0.026*blue + 0.026*pink'), (6, u'0.083*day + 0.029*parti + 0.029*love + 0.015*plan + 0.015*happi'), (7, u'0.004*plan + 0.004*b + 0.004*ton + 0.004*either + 0.004*blue'), (8, u'0.004*plan + 0.004*b + 0.004*space + 0.004*ton + 0.004*blue'), (9, u'0.072*year + 0.072*old + 0.025*twin + 0.025*breast + 0.025*lump')]\n"
     ]
    }
   ],
   "source": [
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary, passes=20)\n",
    "\n",
    "print(ldamodel.print_topics(num_topics=10, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
